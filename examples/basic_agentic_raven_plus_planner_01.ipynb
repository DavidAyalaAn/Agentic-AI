{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d43b7d",
   "metadata": {},
   "source": [
    "# Agentic AI with Nexus Raven and Deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b2ecb",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "This notebook explores the capabilities of an agentic AI system operating under low hardware specifications using locally hosted models.\n",
    "\n",
    "The architecture involves three main components:\n",
    "- Function Caller / Executor: [Nexus Raven], a language model specifically fine-tuned for function calling tasks.\n",
    "- Planner / Assistant: [DeepSeek Coder 6.7B Q4], a quantized LLM responsible for generating step-by-step instructions and answering user queries.\n",
    "- Orchestrator: A coordinating class that integrates both models and manages the end-to-end workflow.\n",
    "\n",
    "\n",
    "System Workflow\n",
    "\n",
    "The interaction flow follows this sequence:\n",
    "1. User submits a query.\n",
    "2. Orchestrator receives and routes the request.\n",
    "3. Planner generates a structured instruction plan.\n",
    "4. Executor runs the function(s) defined in the current instruction.\n",
    "5. Planner updates the next instruction if needed (e.g., based on dynamic values).\n",
    "6. Planner generates the final answer for the user.\n",
    "\n",
    "\n",
    "Observations\n",
    "\n",
    "- Function Caller (Nexus Raven)\n",
    "    - Performs very well with single function calls — responses are accurate and well-formatted.\n",
    "    - Inference time is slow, which could be a limiting factor for real-time applications.\n",
    "    - Performance may degrade when handling a large number of tools or complex multi-step requests.\n",
    "\n",
    "- Planner (DeepSeek Coder 6.7B Q4)\n",
    "    - Also suffers from long inference times, and occasionally crashes the container during processing.\n",
    "    - Struggles to generalize across diverse question types, reducing its reliability as a standalone planner.\n",
    "\n",
    "\n",
    "Use Case\n",
    "\n",
    "The demonstration simulates an AI assistant interacting with a hospital medical record system, capable of retrieving and reasoning over patient data through function execution and contextual planning.\n",
    "\n",
    "Results\n",
    "\n",
    "For low resources is better to use a paid llm, which will allow to use just one llm for function calling and also planner components.\n",
    "With more resources, the inference time is going to be the key so it should be meassured for complex requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ae6d8",
   "metadata": {},
   "source": [
    "# General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fb938",
   "metadata": {},
   "source": [
    "## Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cf13f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Executor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "#Planner\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec11090",
   "metadata": {},
   "source": [
    "# Model Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e6e99",
   "metadata": {},
   "source": [
    "### Function Caller Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0141dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_host = \"http://host.docker.internal:11434\"\n",
    "ollama_model = 'nexusraven:latest'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fdea4",
   "metadata": {},
   "source": [
    "### Planner Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49791509",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_model_path = \"/app/models/deepseek-6.7b/deepseek-coder-6.7b-instruct.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb818d1e",
   "metadata": {},
   "source": [
    "# Context and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769e48e",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9988b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {\n",
    "    'patients' : {\n",
    "        'A00001' : {\n",
    "            'patient_id':'A00001',\n",
    "            'dni':'74324694A',\n",
    "            'name':'Fulgoroncio',\n",
    "            'age':35,\n",
    "            'clinical_records': [\n",
    "                {\n",
    "                    'record_id':'V00001',\n",
    "                    'date':'2024-11-15',\n",
    "                    #'patient_id': 'A00001',\n",
    "                    'disease_id': 'X00005',\n",
    "                    'treatment': 'Buy a hammer and carry everywhere to break the copy reference.',\n",
    "                    'status':'Healed',\n",
    "                    'observation':'She may need another solution in prision.'\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        'A00002' : {\n",
    "            'patient_id':'A00002',\n",
    "            'dni':'24336634A',\n",
    "            'name':'Petunia',\n",
    "            'age':27,\n",
    "            'clinical_records':[\n",
    "                {\n",
    "                    'record_id':'V00001',\n",
    "                    'date':'2024-11-15',\n",
    "                    #'patient_id': 'A00002',\n",
    "                    'disease_id': 'X00005',\n",
    "                    'treatment': 'Buy a hammer and carry everywhere to break the copy reference.',\n",
    "                    'status':'Healed',\n",
    "                    'observation':'She may need another solution in prision.'\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        'A00003' : {\n",
    "            'patient_id':'A00003',\n",
    "            'dni':'33117534B',\n",
    "            'name':'Laura',\n",
    "            'age':36,\n",
    "            'clinical_records': [\n",
    "                {\n",
    "                    'record_id':'V00003',\n",
    "                    'date':'2025-05-01',\n",
    "                    #'patient_id':'A00003',\n",
    "                    'disease_id': 'X00001',\n",
    "                    'status':'Ongoing',\n",
    "                    'treatment': 'Dress with velcro and cover all the toys with velcro.',\n",
    "                    'observation':'She is developing affection for inclusive toys.'\n",
    "                },\n",
    "                {\n",
    "                    'record_id':'V00004',\n",
    "                    'date':'2025-05-10',\n",
    "                    #'patient_id':'A00003',\n",
    "                    'disease_id': 'X00002',\n",
    "                    'status':'Ongoing',\n",
    "                    'treatment': 'Buy a bed instead.',\n",
    "                    'observation': 'She resists to heal. Some penguins are loosing their homes.'\n",
    "                },\n",
    "                {\n",
    "                    'record_id':'V00005',\n",
    "                    'date':'2025-05-25',\n",
    "                    #'patient_id':'A00003',\n",
    "                    'disease_id': 'X00003',\n",
    "                    'status':'Ongoing',\n",
    "                    'treatment': 'Installation of an hamaca in the other room.',\n",
    "                    'observation': 'The patien wants to buy a bigger mattress. She is not following the treatment.'\n",
    "                },\n",
    "                {\n",
    "                    'record_id':'V00006',\n",
    "                    'date':'2025-06-01',\n",
    "                    #'patient_id':'A00003',\n",
    "                    'disease_id': 'X00004',\n",
    "                    'status':'Ongoing',\n",
    "                    'treatment': 'Fire all greeks and indians, literaly and figurative.',\n",
    "                    'observation': 'Patien started with greek people.'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'diseases' : {\n",
    "        'X00001' : {\n",
    "            'disease_name' : 'Invisible toy disorder',\n",
    "            'description': 'The person tends to loose members of toys. Also suffers from cognitive time jumps, as she forgots 10 minute lapses.'\n",
    "        },\n",
    "        'X00002' : {\n",
    "            'disease_name' : 'Sofacosis syndrome',\n",
    "            'description': 'Mental syndrome to get sofas placed in an exact location on images.'\n",
    "        },\n",
    "        'X00003' : {\n",
    "            'disease_name' : 'Spontaneous mattress somnambulism',\n",
    "            'description': 'The person awakes in another room, with a matrees moved from a room to another. In some cases the person develops a phobia to hear possible solutions.'\n",
    "        },\n",
    "        'X00004' : {\n",
    "            'disease_name' : 'Indian immune repulsive',\n",
    "            'description': 'The person naturally causes indian people to vanish or avoid responses to her inqueries. on its chronicle phase it generates unpleasent behaviors from greeks'\n",
    "        },\n",
    "        'X00005' : {\n",
    "            'disease_name' : 'Mirror Mimic Madness',\n",
    "            'description': 'The person believes they are a mirror and must copy the exact movements of whoever is in front of them—even strangers in public.'\n",
    "        },\n",
    "        'X00006' : {\n",
    "            'disease_name' : 'Dramatic Slow-Mo Virus',\n",
    "            'symptoms': 'The infected person perceives their life as a dramatic movie and moves in slow motion during mundane tasks.'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f49ef",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b101efb",
   "metadata": {},
   "source": [
    "### Pydantic Data Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517739b",
   "metadata": {},
   "source": [
    "Pydantic classes are used only to create the definition for the arguments.\n",
    "\n",
    "In this case we also add tags ['...'] to descriptions to give more relevant information to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d325b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hospital_patients(BaseModel):\n",
    "    pass\n",
    "    \n",
    "class patient_clinical_record(BaseModel):\n",
    "    patient_id: str = Field(description='[patiend-identifier] System identifier of the patient.')\n",
    "    \n",
    "class disease_creation(BaseModel):\n",
    "    disease_name: str = Field(description='[disease-name] Name of the disease to be created.')\n",
    "    description: str = Field(description='[disease-description] Description of the disease to be created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e1b18",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c7a000",
   "metadata": {},
   "source": [
    "The tool tag is used to include the class descriptions to the functions. This option allows to create the scheme in a more organized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e57d5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema = hospital_patients)\n",
    "def get_hospital_patients() -> dict:\n",
    "    \"\"\"\n",
    "    [all-patients] This function retrieves the patients registered in the hospital.\n",
    "\n",
    "    Returns: A dictionary of the patients registered in the hospital.\n",
    "    \"\"\"\n",
    "    \n",
    "    patients = {\n",
    "        key: { attr:attr_data for attr,attr_data in data.items() if attr != 'clinical_records' }\n",
    "        for key,data in context['patients'].items()\n",
    "    }\n",
    "\n",
    "    return {'patients': patients}\n",
    "\n",
    "@tool(args_schema = patient_clinical_record)\n",
    "def get_patient_clinical_record(patient_id:str) -> dict:\n",
    "    '''\n",
    "    [clinical-info] Retrieves the clinical history of a patient by the sistem identifier, which has a numerical text portion.\n",
    "    \n",
    "    Returns: Dictionary with the clinical record of the requested patient.\n",
    "    \n",
    "    Args description:\n",
    "        patient_id (str): [patiend-identifier] System identifier of the patient.\n",
    "    '''\n",
    "    \n",
    "    if  patient_id in context['patients'].keys():\n",
    "        patient = context['patients'].get(patient_id)\n",
    "        patient_records = {key:data for key,data in patient.items() if key in ['patient_id','name','clinical_records']}\n",
    "        records = patient_records['clinical_records']\n",
    "        for record in records:\n",
    "            record['disease_name']=context['diseases'].get(record['disease_id'])['disease_name']\n",
    "\n",
    "        return patient_records\n",
    "    else:\n",
    "        print(\"No patient found with id {}\".format(patient_id))\n",
    "        return None\n",
    "\n",
    "@tool(args_schema = disease_creation)\n",
    "def create_disease(disease_name:str, description:str) -> dict:\n",
    "    '''\n",
    "    [disease] Creates a new disease on the system.\n",
    "    \n",
    "    Returns: Dictionary with the new created disease.\n",
    "    '''\n",
    "    diseases = context['diseases']\n",
    "    disease = {}\n",
    "    disease['disease_name'] = disease_name\n",
    "    disease['description'] = description\n",
    "    \n",
    "    new_id = len(diseases.items())\n",
    "    \n",
    "    diseases['X'+str(new_id+1).zfill(5)] = disease\n",
    "    return diseases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84086585",
   "metadata": {},
   "source": [
    "### Tool Schema Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de177813",
   "metadata": {},
   "source": [
    "This tool list will be used to create the prompt corresponding to the tools and to restrict the execution of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d2335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    get_hospital_patients,\n",
    "    get_patient_clinical_record,\n",
    "    create_disease\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f904540",
   "metadata": {},
   "source": [
    "# Agent Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c5033",
   "metadata": {},
   "source": [
    "This agent will be in charge of giving format to the functions for each request.\n",
    "The response form the agent will be used to execute each function and return the response to the planner agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec8435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.tools.base import BaseTool\n",
    "import re\n",
    "import ast\n",
    "\n",
    "class ExecutorAgent:\n",
    "    \n",
    "    INSTRUCTION_PROMPT = '''Instructions:\n",
    "- ONLY use the provided functions.\n",
    "- DO NOT nest function calls.\n",
    "- Use ONLY values that have already been returned in previous steps.\n",
    "- Use the exact function name and parameters as defined.\n",
    "- If no parameters are needed, call without arguments\n",
    "- If the instruction does not include a function call, do not call anything.\n",
    "- Output format must be exactly: Call: function_name(arguments)<bot_end>\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    def __init__(self,tools:List[BaseTool]):\n",
    "        #Tools\n",
    "        self.function_map = {tool.name:tool for tool in tools}\n",
    "        self.tools_prompt = self.build_tools_prompt()\n",
    "\n",
    "        #Prompt\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.tools_prompt),\n",
    "            MessagesPlaceholder(variable_name='chat_history'),\n",
    "            ('user','{user_query}')\n",
    "        ])\n",
    "        \n",
    "        #Model setup        \n",
    "        self.model = ChatOllama(\n",
    "            model=\"nexusraven:latest\",\n",
    "            base_url=ollama_host,\n",
    "            temperature=0.001\n",
    "        )\n",
    "        \n",
    "        self.chat_history = []\n",
    "        self.intermedium_steps = []\n",
    "        self.chain = self.prompt | self.model | RunnableLambda(self.parse_functions)\n",
    "        \n",
    "        print('Executor initialized')\n",
    "    \n",
    "        \n",
    "        \n",
    "    def invoke(self,user_query:str) -> List[Any]:\n",
    "        print('Request sent to executer')\n",
    "        response = self.chain.invoke({\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"user_query\": user_query\n",
    "        })\n",
    "\n",
    "        self.chat_history.append(HumanMessage(content=user_query))\n",
    "        self.chat_history.extend(self.intermedium_steps)\n",
    "        self.chat_history.append(AIMessage(content=\"\\n\".join(map(str, response['result']))))\n",
    "        self.intermedium_steps = []\n",
    "        \n",
    "        return response['result']\n",
    "    \n",
    "    \n",
    "    def clean_history(self) -> None:\n",
    "        self.chat_history = []\n",
    "        self.intermedium_steps = []\n",
    "        \n",
    "    def build_tools_prompt(self) -> str:\n",
    "        '''This function generates the prompt with the available functions'''\n",
    "        \n",
    "        function_prompt = ''\n",
    "        for funct in self.function_map.values():  \n",
    "            function_prompt += ( 'def {tool_name}({args}):\\n\"\"\"\\n{desc}\\n\"\"\"\\n\\n'.format(\n",
    "                tool_name = funct.name, \n",
    "                args = ','.join(['{}:{}'.format(key,data['type']) for key, data in funct.args.items() ]),\n",
    "                desc = funct.description\n",
    "            ))\n",
    "            \n",
    "        system_prompt = function_prompt + self.INSTRUCTION_PROMPT\n",
    "\n",
    "        return system_prompt\n",
    "    \n",
    "    def parse_functions(self, response:AIMessage) -> List[Any]:\n",
    "        '''This function is used to:\n",
    "            - extract the function and arguments from the response.\n",
    "            - run the function.\n",
    "            - return the result from each function.\n",
    "        '''\n",
    "        \n",
    "        response_message = response.content\n",
    "        self.intermedium_steps.append(AIMessage(content=response.content))\n",
    "        \n",
    "        #We look for function calls located between the strings \"Call:\" and \"<bot_end>\"\n",
    "        matches = self.extract_function_calls(response_message)\n",
    "\n",
    "        print(matches)\n",
    "        #We assure we got function calls in the response\n",
    "        results = []\n",
    "        if matches:\n",
    "            #This loop runs througth the sequence of functions included in the response\n",
    "            for func_name, args_text in matches:\n",
    "                try:\n",
    "                    #This executes our functions\n",
    "                    args = ast.literal_eval(f'dict({args_text})') if (args_text.strip()) and (args_text != 'null=null') else {}\n",
    "                    self.intermedium_steps.append(AIMessage(content='Called function: {}({})'.format(func_name,args)))\n",
    "                    print('Called function: {}({})'.format(func_name,args))\n",
    "                    \n",
    "                    result = self.function_map[func_name].run(args)\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    error_message = 'Function call failed: {}({}) - {}'.format(func_name, args, str(e))\n",
    "                    results.append(error_message)\n",
    "        else:\n",
    "            results.append(\"No function calls found.\")\n",
    "        return {'result':results}\n",
    "    \n",
    "    def get_chat_history(self) -> list[Any]:\n",
    "        '''This function retrieves the chat history'''\n",
    "        return self.chat_history\n",
    "    \n",
    "    def extract_function_calls(self, message:str) -> List[tuple]:\n",
    "        #This function is used to extract the functions in the response\n",
    "        return re.findall(r'Call:\\s*(\\w+)\\((.*?)\\)<bot_end>', message)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5e2a8",
   "metadata": {},
   "source": [
    "# Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59c67f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerAgent:\n",
    "    \n",
    "    REQUEST_WRAPPER_TEMPLATE = \"### Instruction:\\n{request_prompt}.\\n### Response:\"\n",
    "    \n",
    "    REQUEST_PLAN_TEMPLATE = \"\"\"You are a strict planner for a function-calling agent.\n",
    "ONLY use the following allowed functions:\n",
    "{tools}\n",
    "When creating the plan for an llm, you have to make a logic step by step list, under the following considerations:\n",
    "- DO NOT guess values like patient_id unless they were previously retrieved.\n",
    "- If the user query requires unknown information, return no call.\n",
    "- Order the steps from the first to the last.\n",
    "- Only just one non function calling line between function calling lines.\n",
    "- You MUST output a function call in this format (only if valid):\n",
    "    Call: function_name(argument=<result_from_step_2>)\n",
    "- Only answer the request sorrounded by ***\n",
    "\n",
    "User query: ***{user_query}***\n",
    "\"\"\"\n",
    "            \n",
    "    \n",
    "    def __init__(self, tools:List[BaseTool]):\n",
    "        self.function_map = {tool.name:tool for tool in tools}\n",
    "        self.tools_prompt = self.build_tools_prompt()\n",
    "        \n",
    "        self.model = Llama(\n",
    "            model_path=planner_model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_ctx=4096,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        print('Planner initialized')\n",
    "        \n",
    "    def invoke(self,request_prompt:str) -> str:\n",
    "        response = self.model(\n",
    "            request_prompt,\n",
    "            max_tokens=256,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        response_text = response['choices'][0]['text'].strip()\n",
    "        separator = response_text.find('***')\n",
    "        response_text = response_text[:(separator if separator != -1 else len(response_text))]\n",
    "        \n",
    "        return response_text\n",
    "    \n",
    "    def invoke_mode(self,user_query:str,planner_mode:bool=True) -> str:\n",
    "        \n",
    "        request_prompt = (\n",
    "            self.format_prompt_plan(user_query) \n",
    "            if planner_mode else \n",
    "            self.format_prompt_request(user_query)\n",
    "        )\n",
    "\n",
    "        response_text = self.invoke(request_prompt)\n",
    "        \n",
    "        return response_text       \n",
    "    \n",
    "    def format_prompt_plan(self, user_query:str) -> str:\n",
    "        request_prompt = self.REQUEST_PLAN_TEMPLATE.format(tools=self.tools_prompt, user_query=user_query)\n",
    "        return self.format_prompt_request(request_prompt)\n",
    "    \n",
    "    \n",
    "    def format_prompt_request(self, request_prompt:str) -> str:\n",
    "        return self.REQUEST_WRAPPER_TEMPLATE.format(request_prompt=request_prompt)\n",
    "    \n",
    "    def build_tools_prompt(self) -> str:\n",
    "        \n",
    "        function_prompt = ''\n",
    "        for funct in self.function_map.values():  \n",
    "            function_prompt += ( '- {tool_name}({args})\\n    {desc}\\n'.format(\n",
    "                tool_name = funct.name, \n",
    "                args = ','.join(['{}:{}'.format(key,data['type']) for key, data in funct.args.items() ]),\n",
    "                desc = funct.description[:funct.description.find('\\n')+1]\n",
    "            ))\n",
    "\n",
    "        return function_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0300ec2",
   "metadata": {},
   "source": [
    "# Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orchestrator:\n",
    "    \n",
    "    PROMPT_ASSISTANT_REVIEW_TEMPLATE = '''You wrote this instruction:\n",
    "{instruction}\n",
    "                \n",
    "Here is the result of the previous step:\n",
    "{previous_output}\n",
    "                \n",
    "Please rewrite the next instruction using the actual values:\n",
    "{posterior_instruction}\n",
    "'''\n",
    "                \n",
    "    PROMPT_ASSISTANT_ANSWER_TEMPLATE = '''You are an intelligent assistant.\n",
    "Here is the data you need to use:\n",
    "{data_requested}\n",
    "\n",
    "Based on this data, answer the user query enclosed in triple asterisks.\n",
    "\n",
    "Your response must:\n",
    "- Use only the information from the data above.\n",
    "- Do not use any \"id\" fields. That means: DO NOT include anything labeled with “record_id”, “disease_id”, or similar. They must be completely excluded from the output.\n",
    "- Do not use programable code to answer.\n",
    "\n",
    "*** {user_query} ***\n",
    "'''\n",
    "    \n",
    "    \n",
    "    def __init__(self,planner:PlannerAgent,executor:ExecutorAgent):\n",
    "        self.planner = planner\n",
    "        self.executor = executor\n",
    "        self.chat_history = []\n",
    "        print('Orchestrator Initialized')\n",
    "    \n",
    "    def llm_chat(self,user_query:str) -> str:\n",
    "        self.chat_history.append({'role':'user','content':user_query})\n",
    "        response = self.planner.invoke_mode(user_query,planner_mode=True)\n",
    "        print(response)\n",
    "        instructions = self.plan_splitter(response)\n",
    "        self.chat_history.append({'role':'planner','content':'\\n'.join(instructions)})\n",
    "        print('\\n'.join(instructions))\n",
    "        \n",
    "        for i in range(len(instructions)):\n",
    "            step = instructions[i]\n",
    "            print('Working on step:',step)\n",
    "            self.chat_history.append({'role':'planner','content':step})\n",
    "            \n",
    "            if self.is_function_call_instruction(step):\n",
    "                self.executor.clean_history()\n",
    "                response = self.executor.invoke(step)\n",
    "                self.chat_history.append({'role':'executor','content':response})\n",
    "            else:                \n",
    "                prompt = self.prompt_assistant_review(\n",
    "                    instruction=step, \n",
    "                    previous_output=self.chat_history[-2]['content'], \n",
    "                    posterior_instruction=instructions[i+1]\n",
    "                )\n",
    "                \n",
    "                response = self.planner.invoke_mode(prompt,planner_mode=False)\n",
    "                instructions[i+1] = self.instruction_extraction(response)\n",
    "                self.chat_history.append({'role':'planner','content':response})\n",
    "\n",
    "        prompt = self.prompt_assistant_answer(\n",
    "            data_requested=self.chat_history[-1]['content'], \n",
    "            user_query=user_query\n",
    "        )\n",
    "        \n",
    "        self.chat_history.append({'role':'planner','content':response})\n",
    "        response = self.planner.invoke_mode(prompt,planner_mode=False)\n",
    "        self.chat_history.append({'role':'assistant','content':response})\n",
    "        \n",
    "        return response\n",
    "        \n",
    "\n",
    "    def plan_splitter(self,response: str) -> list[str]:\n",
    "        plan_response = response[response.find('Call:')-3:response.find('\\n',response.rfind('Call:'))]\n",
    "        instructions = plan_response.split('\\n')\n",
    "        return instructions\n",
    "    \n",
    "    def is_function_call_instruction(self, instruction: str) -> bool:\n",
    "        \"\"\"Check if the instruction is of the form 'N. Call: function_name(...)'.\"\"\"\n",
    "        print('Evaluating type of instruction in:',instruction)\n",
    "        return bool(re.match(r\"^\\s*\\d+\\.\\s*Call:\\s*\\w+\\(.*\\)\", instruction.strip()))\n",
    "    \n",
    "    def instruction_extraction(self,response:str) ->str:\n",
    "        breakline_index = response.find('\\n',response.rfind('Call:'))\n",
    "        instruction = response[response.find('Call:')-3:(breakline_index + 1 if breakline_index != -1 else None)]\n",
    "        return instruction\n",
    "    \n",
    "    def get_chat_history(self) -> list[Dict[str:Any]]:\n",
    "        return self.chat_history\n",
    "    \n",
    "    def prompt_assistant_review(self,instruction:str,previous_output:str,posterior_instruction:str) -> str:\n",
    "        return self.PROMPT_ASSISTANT_REVIEW_TEMPLATE.format(instruction=instruction, previous_output=previous_output, posterior_instruction=posterior_instruction)\n",
    "    \n",
    "    def prompt_assistant_answer(self,data_requested:str,user_query:str) -> str:\n",
    "        return self.PROMPT_ASSISTANT_ANSWER_TEMPLATE.format(data_requested=data_requested, user_query=user_query)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa568a",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31c2c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner initialized\n",
      "Orchestrator Initialized\n"
     ]
    }
   ],
   "source": [
    "exec_agent = ExecutorAgent(tools)\n",
    "plan_agent = PlannerAgent(tools)\n",
    "orchestrator_chat = Orchestrator(plan_agent, exec_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbff97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patients in the data provided are:\n",
      "\n",
      "1. Fulgoroncio, a 35-year-old with a DNI of 74324694A and patient ID A00001.\n",
      "2. Petunia, a 27-year-old with a DNI of 24336634A and patient ID A00002.\n",
      "3. Laura, a 36-year-old with a DNI of 33117534B and patient ID A00003.\n",
      "\n",
      "Please note that this information is based on the data provided and does not include any \"id\" fields.\n"
     ]
    }
   ],
   "source": [
    "resp = orchestrator_chat.llm_chat('Show me all the patiens')\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
